{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11cb8449",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Installieren aller Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173ff31",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32adcf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4a3e4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from time import sleep\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "import random, datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from utils.agent import Mario\n",
    "from utils.wrappers import ResizeObservation, SkipFrame\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import random, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "from nes_py.wrappers import JoypadSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64205536",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919632d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    action_idx (int): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e63b6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (LazyFrame),\n",
    "        next_state (LazyFrame),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done(bool))\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "\n",
    "        if self.use_cuda:\n",
    "            state = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action = torch.tensor([action]).cuda()\n",
    "            reward = torch.tensor([reward]).cuda()\n",
    "            done = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action = torch.tensor([action])\n",
    "            reward = torch.tensor([reward])\n",
    "            done = torch.tensor([done])\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd2122",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"mini cnn structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ec756",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26104383",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aed585",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "    def load(self, load_dir):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12dc63b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a39d2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252d416",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hot Topic: Reinforcement Learning\n",
    "\n",
    "\n",
    "<img width=70% src=\"../images/pawel-czerwinski-oNcFi_dYz_0-unsplash.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2113743",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In diesem Exkurs werden wir eine kurze Einführung in Reinforcement Learning geben und anhand eines praktischen Beispiels die Funktionsweise kurz aufzeigen. Wir verwenden hierbei **OpenAI Gym**, einer Entwicklungsplatform für Reinforcement Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b3565",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wiederholung Reinforcement Learning\n",
    "\n",
    "<img width=70% src=\"../images/reinforcement.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e68eb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reinforcement Learning ist eines der vier Paradigmen im Machine Learning neben Supervised, Unsupervised und Semi-Supervised Learning. Im Reinforcement Learning agiert ein Agent mit Aktionen mit der Umwelt. Die Umwelt liefert wiederum Belohnungen und den aktuellen Stand der Umwelt als Beobachtung. Als Lernprozess versucht man die sogenannte Policy/Strategie des Agenten anzupassen und zu verbessern.\n",
    "\n",
    "Grundsätzlich unterscheidet man zwischen **Exploration** und **Exploitation** im Training. Einerseits will man bisherige Strategien verbessern, aber auch zu einem gewissen Teil neue Strategien austesten. Das wird meist mit einem Parameter spezifiziert. Zu Beginn ist die Exploration deutlich ausgeprägter als bei fortgeschrittenem Training.  \n",
    "\n",
    "In dem Einsteiger Kurs \"Künstliche Intelligenz und maschinelles Lernen für Einsteiger\" sind wir in einer Einheit etwas genauer auf Reinforcement Learning eingegangen. [Link hier.](https://open.hpi.de/courses/kieinstieg2020/items/3OBaGkP33fILA2t8FwqiC1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2638ff1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OpenAI Gym Environment\n",
    "\n",
    "\n",
    "<img width=50% src=\"../images/openai-cover.png\">\n",
    "\n",
    "In unserer Einheit nutzen wir OpenAI Gym. OpenAI Gym ist ein Open Source Umgebung für die Entwicklung von Reinforcement Learning Modellen für das es eine Vielzahl sogenannter Environments gibt. Darunter sind Atari-Spiele, Nintendo-Spiele sowie Umgebungen mit Robotern, die es in der Realität zu kaufen gibt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc836eae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hier mal ein Ausschnitt mehrerer Umgebungen. \n",
    "\n",
    "<img width=70% src=\"../images/environments.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce687d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SuperMario Bros. Umgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e463b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Im ersten Schritt wollen wir uns einmal die Umgebung ansehen und auch graphisch ausgeben lassen. Wir haben uns hier für das bekannte Nintendo-Spiel SuperMario Bros. entschieden als Environment. Die Environment wird als [OpenSource Projekt](https://github.com/Kautenja/gym-super-mario-bros) von anderen EntwicklerInnen zur Verfügung gestellt.\n",
    "\n",
    "Im ersten Schritt definieren wir die Umgebung und anschließend führen für eine gewisse Anzahl von Schritten jeweils zufällige Aktionen aus. Hierbei findet noch kein Lernvorgang statt - wir wollen nur erst einmal die Umgebung zeigen. \n",
    "\n",
    "Um die Ausführung zu unterbrechen, kann man im Menü oben die aktuelle Zellenausführung stoppen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd7ce4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "done = True\n",
    "for step in range(1000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(.05)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01c928",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Für unser richtiges Reinforcement Learning Modell müssen wir allerdings noch weitere Schritte ausführen. \n",
    "\n",
    "Zunächst sehen wir uns einmal an, welche möglichen Actions wir zur Verfügung haben in dieser Environment. Wir werden das anschließend nur auf zwei Actions limitieren, um schneller zu trainieren. Wir verwenden dann nur noch **right** (walk right) und **right, A** (jump right). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665ad9f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
    "gym_super_mario_bros.actions.COMPLEX_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dae592",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d35ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wollen wir erfahren, was wir von unserer Environment als Informationen bekommen, so geben wir uns einmal die Rückgabewerte der Umgebung aus. Hier sieht man jedoch, dass wir nur relativ wenige Informationen direkt bekommen. Um unser Modell richtig trainieren zu können, müssen wir den aktuellen Stand eines Spiels jeweils als Bild erhalten.\n",
    "\n",
    "Unser Modell lernt anhand der Bilder des aktuellen Spielstandes. Dabei werden sogenannte Convolutional Neural Networks genutzt, die wir in Woche 4 etwas genauer betrachten werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f8050",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(\"Next-State: \" + str(next_state.shape))\n",
    "print(\"Reward: \" + str(reward))\n",
    "print(\"Done: \" + str(done))\n",
    "print(\"Info: \" + str(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0c4a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Damit wir Feedback der Environment auch in Form von \"Bildern\" bekommen, müssen wir folgendes tun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11a3d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f91677",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning\n",
    "\n",
    "Im Folgenden werden wir den Q-Learning Ansatz verwenden. Beim Q-Learning kann der Agent die Belohnungen der Umgebung nutzen, um im Laufe der Zeit zu lernen, welche Aktion in einem bestimmten Zustand der Environment am besten ist. Weiter werden wir hier nicht ins Detail gehen. \n",
    "\n",
    "Die Implementierung und finales Modell stammt von [YuansongFeng](https://github.com/YuansongFeng/MadMario) und einem [Pytorch-Tutorial](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a95a2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Die ersten Schritte beim Lernen\n",
    "\n",
    "Wir wollen uns zunächst einmal ansehen, wie unser Modell sich ohne Training schlägt. Anschließend werden wir mehrere bereits trainierte Modell laden und uns verschieden gute Modell ansehen. \n",
    "\n",
    "In der Präsentation werden wir das nicht zeigen, doch wir haben in den Notebooks auch den Code zum Training der einzelnen Modelle. Da das sehr zeitintensiv ist, werden wir nur bereits trainierte Modelle verwenden. Das Training geschieht in sogenannten Episoden. Das Training von 1.000 Episoden kann bereits mehrere Stunden dauern. Für ein \"wirklich gutes\" Modell benötigt man mehrere zehntausend Episoden Training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19ac8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=\"\")\n",
    "\n",
    "episodes = 100\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        time.sleep(.05)\n",
    "\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent führt Aktion aus und bekommt dafür Feedback der Environment\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Mario merkt sich den aktuellen Stand\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "        \n",
    "        # Mario führt einen Lernschritt aus\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcec497",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Training 100 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295828a5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "save_dir = Path(\"checkpoints_100\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "\n",
    "\n",
    "episodes = 100\n",
    "for e in range(episodes):\n",
    "    if e % 10 == 0:\n",
    "        print(\"Episode: \" + str(e))\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
    "mario.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a3cd7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Training 1000 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a233d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Start Training\n",
    "save_dir = Path(\"checkpoints_1000\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 1000\n",
    "for e in range(episodes):\n",
    "    if e % 100 == 0:\n",
    "        print(\"Episode: \" + str(e))\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
    "mario.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917a4af",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Training 10.000 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc7386",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "save_dir = Path(\"checkpoints_10000\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 10000\n",
    "for e in range(episodes):\n",
    "    if e % 10000 == 0:\n",
    "        print(\"Episode: \" + str(e))\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 10000 == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd37e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Trainierte Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b95c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nachdem wir uns unseren Agenten einmal komplett ohne Training angesehen haben, so wollen wir uns einmal das Verhalten nach 100 Episoden ansehen. Vorher muss zuerst allerdings die Environment zurückgesetzt werden, dass wir die Modelle laden können. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499525c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from utils.agent import Mario\n",
    "from utils.wrappers import ResizeObservation, SkipFrame\n",
    "\n",
    "# Initialize Super Mario environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(\n",
    "    env,\n",
    "    [['right'],\n",
    "    ['right', 'A']]\n",
    ")\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c719e88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modell mit 100 Episoden Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34090fa7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = Path('models/mario_100ep.chkpt')\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=\"\", checkpoint=checkpoint)\n",
    "mario.exploration_rate = mario.exploration_rate_min\n",
    "\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(.05)\n",
    "\n",
    "        action = mario.act(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done or info['flag_get']:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df58ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modell mit 1000 Episoden Training\n",
    "\n",
    "Nachdem unser Modell mit 100 Episoden noch keine großartigen Fortschritte zu verzeichnen hat, wollen wir uns mal das Modell nach circa 1000 Episoden ansehen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a954817",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = Path('models/mario_1000ep.chkpt')\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=\"\", checkpoint=checkpoint)\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(.05)\n",
    "\n",
    "        action = mario.act(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done or info['flag_get']:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff203cee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fertiges Modell\n",
    "\n",
    "Wenn wir nun uns ein \"fertiges\" Modell einmal ansehen, sehen wir deutlich besseres Verhalten und höheren Reward (Erreichen des Ziels, Münzen, kürzere Zeiten etc.). Das hier gezeigte Modell wurde für mehrere zehntausend Episoden trainiert - ist aber noch keinesfalls perfekt.  \n",
    "\n",
    "Natürlich trainieren wir hier auf sehr einfachen Levels des Spiels. Zudem haben wir zu Beginn die Aktionen nur auf zwei beschränkt. Gibt man dem Agenten mehrere Aktionen zur Auswahl und mehr Episoden im Training erhalten wir sicherlich noch ein besseres Modell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7fe4df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = Path('models/mario_trained.chkpt')\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=\"\", checkpoint=checkpoint)\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(.05)\n",
    "\n",
    "        action = mario.act(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done or info['flag_get']:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ff0a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Das war es zum Exkurs Reinforcement Learning. Falls Sie Reinforcement Learning oder einige der Anwendungsfälle interessieren, so können Sie mit https://gym.openai.com/ sehr einfach damit starten. \n",
    "\n",
    "Viel Spaß beim selbst ausprobieren. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "ed87a78f94bed40a9a6d5b36ab069eef020be8c3b14b827d69a52d67089a86b3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('openhpi-kipraxis': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
